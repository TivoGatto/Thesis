{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InfoVAE MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZmwrAKUdz0aKSGQCqn+Km",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TivoGatto/Thesis/blob/master/InfoVAE/InfoVAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lczZXY23xkEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LIBRARIES\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, ReLU, Dense, Flatten, Reshape, Conv2DTranspose, Lambda\n",
        "from keras.datasets import mnist\n",
        "import keras.backend as K"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9FlhcPkx2zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "input_dim = (32, 32, 1)\n",
        "latent_dim = 16\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 100\n",
        "\n",
        "alpha = 0\n",
        "lam   = 1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-x4GKGtx7fI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADDITIONAL FUNCTIONS\n",
        "def vae_loss(z_mean, z_log_var, z_true, z_gen):\n",
        "    def loss(x_true, x_pred):\n",
        "        x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "        x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "        xent_loss = 0.5 * K.sum(K.square(x_true - x_pred), axis=-1)\n",
        "        reg_loss = 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1) # E_x[DKL(q(z|x) || p(z))]\n",
        "        mmd_loss = compute_mmd(z_true, z_gen)\n",
        "\n",
        "        return K.mean(xent_loss + (1 - alpha) * reg_loss) + (alpha + lam - 1) * mmd_loss   # MMD(q(z) || p(z))\n",
        "    return loss\n",
        "\n",
        "def recon(x_true, x_pred):\n",
        "    x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "    x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "    return K.mean(0.5 * K.sum(K.square(x_true - x_pred), axis=-1))\n",
        "\n",
        "def KL(z_mean, z_log_var):\n",
        "    def loss(x_true, x_pred):\n",
        "        return K.mean(0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1))\n",
        "    return loss\n",
        "\n",
        "def mmd_loss(x_true, x_pred):\n",
        "    return compute_mmd(z_true, z_gen)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
        "\n",
        "    return z_mean + epsilon * K.exp(0.5 * z_log_var)\n",
        "\n",
        "def compute_kernel(x, y):\n",
        "    x_size = batch_size\n",
        "    y_size =  x_size\n",
        "\n",
        "    dim     = K.int_shape(x)[1]\n",
        "    tiled_x = K.tile(K.reshape(x, K.stack([x_size, 1, dim])), K.stack([1, y_size, 1])) \n",
        "    tiled_y = K.tile(K.reshape(y, K.stack([1, y_size, dim])), K.stack([x_size, 1, 1]))\n",
        "    return K.exp(-K.mean(K.square(tiled_x - tiled_y), axis=2) / K.cast(dim, 'float32'))\n",
        "\n",
        "def compute_mmd(x, y):\n",
        "    x_kernel = compute_kernel(x, x)\n",
        "    y_kernel = compute_kernel(y, y)\n",
        "    xy_kernel = compute_kernel(x, y)\n",
        "\n",
        "    return K.mean(x_kernel) + K.mean(y_kernel) - 2 * K.mean(xy_kernel) \n",
        "\n",
        "def pad(x, d):\n",
        "    size = x.shape[0]\n",
        "    h, w = x.shape[1:]\n",
        "\n",
        "    x = np.reshape(x, (size, h, w, 1))\n",
        "\n",
        "    x_padded = np.zeros(shape=(size, ) + d)\n",
        "    x_padded[:, :h, :w] = x\n",
        "\n",
        "    return x_padded"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bg8moViyTaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATASET\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = pad(x_train, input_dim) / 255 # For MNIST, we pad x_train and x_test in \n",
        "x_test  = pad(x_test, input_dim) / 255 # shape (32, 32, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print('Train Shape: ', x_train.shape)\n",
        "print('Test Shape: ', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUgcAMYUyZln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Architecture\n",
        "# ENCODER\n",
        "x = Input(shape=input_dim) # Shape (32, 32, 1)\n",
        "\n",
        "h = Conv2D(128, 4, strides=(2, 2), padding='same')(x) # Shape (16, 16, 128)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(256, 4, strides=(2, 2), padding='same')(h) # Shape (8, 8, 256)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(512, 4, strides=(2, 2), padding='same')(h) # Shape (4, 4, 512)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(1024, 4, strides=(2, 2), padding='same')(h) # Shape (2, 2, 1024)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Flatten()(h)\n",
        "\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(x, [z, z_mean, z_log_var])\n",
        "\n",
        "# DECODER\n",
        "z_in = Input(shape=(latent_dim, ))\n",
        "\n",
        "h = Dense(8 * 8 * 1024)(z_in)\n",
        "h = Reshape((8, 8, 1024))(h)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(512, 4, strides=(2, 2), padding='same')(h) # Shape (16, 16, 512)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(256, 4, strides=(2, 2), padding='same')(h)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "x_decoded = Conv2DTranspose(1, 4, strides=(1, 1), padding='same', activation='sigmoid')(h)\n",
        "\n",
        "decoder = Model(z_in, x_decoded)\n",
        "\n",
        "# Generate z samples\n",
        "z_true = K.random_normal(shape=(batch_size, latent_dim), mean=0, stddev=1)\n",
        "z_gen  = encoder(x)[0]\n",
        "\n",
        "# VAE\n",
        "x_recon = decoder(z)\n",
        "\n",
        "vae = Model(x, x_recon)\n",
        "\n",
        "# Compile model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(lr=0.001)\n",
        "\n",
        "vae.compile(optimizer=optimizer, loss=vae_loss(z_mean, z_log_var, z_true, z_gen), metrics=[recon, KL(z_mean, z_log_var)])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU39L0uWyqLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit model\n",
        "hist = vae.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TevtWUt9y1hX",
        "colab_type": "text"
      },
      "source": [
        "# Generation and Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAQt-dk9yzUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reconstruction\n",
        "n = 10\n",
        "digit_size = input_dim[0]\n",
        "\n",
        "x_recon = vae.predict(x_test, batch_size=batch_size)\n",
        "x_recon = np.reshape(x_recon, (-1, digit_size, digit_size))\n",
        "x_test = np.reshape(x_test, (-1, digit_size, digit_size))\n",
        "figure = np.zeros((2 * digit_size, n * digit_size))\n",
        "\n",
        "for i in range(n):\n",
        "    sample = np.random.randint(0, len(x_recon))\n",
        "    figure[:digit_size, i * digit_size: (i+1) * digit_size] = x_test[sample]\n",
        "    figure[digit_size:, i * digit_size: (i+1) * digit_size] = x_recon[sample]\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, ) + input_dim)\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Drh2g2zGbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation\n",
        "n = 10 #figure with n x n digits\n",
        "digit_size = 32\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "# we will sample n points randomly sampled\n",
        "\n",
        "z_sample = np.random.normal(size=(n**2, latent_dim), scale=1)\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        x_decoded = decoder.predict(np.array([z_sample[i + n * j]]))\n",
        "        digit = x_decoded.reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "            j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_vR2Ot2zIgL",
        "colab_type": "text"
      },
      "source": [
        "# Metrics Evaluation\n",
        "\n",
        "First of all, we want to evaluate the ability of the model of generate high quality samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmoGPJr3zJMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FID Score\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "from skimage.transform import resize\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "# Functions needed to compute FID score\n",
        "def scale_images(images, new_shape): # Scale an image in a new shape using NN Interpolation\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn np.asarray(images_list)\n",
        "\n",
        "\n",
        "def calculate_fid(model, images1, images2): # Calculate Frechet Inception Distance between images1, images2\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "\n",
        "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "\tif np.iscomplexobj(covmean): # Check if the sqrtm is complex\n",
        "\t\tcovmean = covmean.real\n",
        "\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "z_sample = np.random.normal(0, 1, size=(sample_size, latent_dim))\n",
        "sample = np.random.randint(0, len(x_test), size=sample_size)\n",
        "x_gen = decoder.predict(z_sample)\n",
        "x_real = x_test[sample]\n",
        "\n",
        "x_gen = evaluate.scale_images(x_gen, (299, 299, 1))\n",
        "x_real = evaluate.scale_images(x_real, (299, 299, 1))\n",
        "print('Scaled', x_gen.shape, x_real.shape)\n",
        "\n",
        "x_gen_t = preprocess_input(x_gen)\n",
        "x_real_t = preprocess_input(x_real)\n",
        "\n",
        "x_gen = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "x_real = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "for i in range(3):\n",
        "    x_gen[:, :, :, i] = x_gen_t[:, :, :, 0]\n",
        "    x_real[:, :, :, i] = x_real_t[:, :, :, 0]\n",
        "print('Final', x_gen.shape, x_real.shape)\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "\n",
        "# fid between images1 and images2\n",
        "fid = evaluate.calculate_fid(model, x_real, x_gen)\n",
        "print('FID (different): %.3f' % fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjB_JERNzMoE",
        "colab_type": "text"
      },
      "source": [
        "### Deactivated Latent Variables, Variance Loss and Variance Law\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wysVr3gszLEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_deactivated_variables(z_var, treshold = 0.8):\n",
        "    z_var = np.mean(z_var, axis=0)\n",
        "\n",
        "    return np.sum(z_var > treshold)\n",
        "\n",
        "def loss_variance(x_true, x_recon):\n",
        "    x_true = np.reshape(x_true, (-1, np.prod(x_true.shape[1:])))\n",
        "    x_recon = np.reshape(x_recon, (-1, np.prod(x_recon.shape[1:])))\n",
        "\n",
        "    var_true = np.mean(np.var(x_true, axis=1), axis=0)\n",
        "    var_recon = np.mean(np.var(x_recon, axis=1), axis=0)\n",
        "\n",
        "    return np.abs(var_true - var_recon)\n",
        "\n",
        "########################################################################################################################\n",
        "# SHOW THE RESULTS\n",
        "########################################################################################################################\n",
        "\n",
        "_, z_mean, z_log_var = encoder.predict(x_test, batch_size=batch_size)\n",
        "z_var = np.exp(z_log_var)\n",
        "n_deact = count_deactivated_variables(z_var)\n",
        "print('We have a total of ', latent_dim, ' latent variables. ', count_deactivated_variables(z_var), ' of them are deactivated')\n",
        "\n",
        "var_law = np.mean(np.var(z_mean, axis=0) + np.mean(z_var, axis=0))\n",
        "print('Variance law has a value of: ', var_law)\n",
        "\n",
        "x_recon = vae.predict(x_train, batch_size=batch_size)\n",
        "print('We lost ', loss_variance(x_test, x_recon), 'Variance of the original data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzk36kzVzRWT",
        "colab_type": "text"
      },
      "source": [
        "### Latent space matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuzFTnwizPW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to verify if q(z) = p(z).\n",
        "\n",
        "# Moments Matching\n",
        "# Generate samples from q(z) and for p(z)\n",
        "# p(z) = N(0, I)\n",
        "# q(z) = E_q(x)[q(z|x)]\n",
        "#\n",
        "# For every moment we compare the log-moments\n",
        "n = len(x_test)\n",
        "\n",
        "p_samples = np.random.normal(size=(n, latent_dim))\n",
        "q_samples = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "\n",
        "from scipy.stats import moment\n",
        "# First moment matching:\n",
        "p_first_moment = np.mean(moment(p_samples, moment=1, axis=0))\n",
        "q_first_moment = np.mean(moment(q_samples, moment=1, axis=0))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"First moment of p(z): \" + str(p_first_moment))\n",
        "print(\"First moment of q(z): \" + str(q_first_moment))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Second moment matching:\n",
        "p_second_moment = np.log(np.mean(moment(p_samples, moment=2, axis=0)))\n",
        "q_second_moment = np.log(np.mean(moment(q_samples, moment=2, axis=0)))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Second log-moment of p(z): \" + str(p_second_moment))\n",
        "print(\"Second log-moment of q(z): \" + str(q_second_moment))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Thid moment matching:\n",
        "p_third_moment = np.log(np.mean(moment(p_samples, moment=3, axis=0)))\n",
        "q_third_moment = np.log(np.mean(moment(q_samples, moment=3, axis=0)))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Third log-moment of p(z): \" + str(p_third_moment))\n",
        "print(\"Third log-moment of q(z): \" + str(q_third_moment))\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}