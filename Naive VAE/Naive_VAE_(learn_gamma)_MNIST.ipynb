{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive VAE (learn gamma) MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMa8fbkLq7L87HWD0itqWfY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TivoGatto/Thesis/blob/master/Naive%20VAE/Naive_VAE_(learn_gamma)_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Heo3fcS5w9VX",
        "colab_type": "text"
      },
      "source": [
        "Implementation of Naive VAE with architecture taken from Tolsikhin et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpxulJY8w5wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LIBRARIES\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, ReLU, Dense, Flatten, Reshape, Conv2DTranspose, Lambda, Add\n",
        "from keras.datasets import mnist\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaf7iCDqxrtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "input_dim = (32, 32, 1)\n",
        "latent_dim = 16\n",
        "\n",
        "epochs = 150\n",
        "batch_size = 100\n",
        "\n",
        "initial_lr = 1e-4\n",
        "halve_at_epoch = 100\n",
        "\n",
        "global beta\n",
        "beta = 1\n",
        "\n",
        "TRAIN = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU0TYFOiySzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions\n",
        "def vae_loss(z_mean, z_log_var):\n",
        "    def loss(x_true, x_pred):\n",
        "        x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "        x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "        L_rec = 0.5 * K.sum(K.square(x_true - x_pred), axis=-1)\n",
        "        L_KL = 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1)\n",
        "        \n",
        "        return K.mean(L_rec + beta * L_KL)\n",
        "    return loss\n",
        "\n",
        "def recon(x_true, x_pred):\n",
        "    x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "    x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "    return K.mean(0.5 * K.sum(K.square(x_true - x_pred), axis=-1))\n",
        "\n",
        "def KL(z_mean, z_log_var):\n",
        "    def kl(x_true, x_pred):\n",
        "        return K.mean(0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1))\n",
        "    return kl\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    eps = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
        "    \n",
        "    return z_mean + K.exp(0.5 * z_log_var) * eps\n",
        "\n",
        "def pad(x, d):\n",
        "    size = x.shape[0]\n",
        "    h, w = x.shape[1:]\n",
        "\n",
        "    x = np.reshape(x, (size, h, w, 1))\n",
        "\n",
        "    x_padded = np.zeros(shape=(size, ) + d)\n",
        "    x_padded[:, :h, :w] = x\n",
        "\n",
        "    return x_padded\n",
        "\n",
        "class UpdateBeta(Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        global beta\n",
        "        val = logs['recon']\n",
        "        beta = val * latent_dim / np.prod(input_dim)\n",
        "\n",
        "        print(' - Beta = ' + str(beta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rofX_33MyUID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "546176a3-9fb0-4f05-c2b8-c2a8b1469063"
      },
      "source": [
        "# Dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = pad(x_train, input_dim) / 255 # For MNIST, we pad x_train and x_test in \n",
        "x_test  = pad(x_test, input_dim) / 255  # shape (32, 32, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "\n",
        "print('x_train shape: ' + str(x_train.shape))\n",
        "print('x_test shape: ' + str(x_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 32, 32, 1)\n",
            "x_test shape: (10000, 32, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3xUD92W1NKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Architecture\n",
        "# ENCODER\n",
        "x = Input(shape=input_dim) # Shape (32, 32, 1)\n",
        "\n",
        "h = Conv2D(32, 4, strides=(2, 2), padding='same')(x) # Shape (16, 16, 32)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(64, 4, strides=(2, 2), padding='same')(h) # Shape (8, 8, 64)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(128, 4, strides=(2, 2), padding='same')(h) # Shape (4, 4, 128)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Flatten()(h)\n",
        "\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(x, [z, z_mean, z_log_var])\n",
        "\n",
        "# DECODER\n",
        "z_in = Input(shape=(latent_dim, ))\n",
        "\n",
        "h = Dense(4 * 4 * 128)(z_in)\n",
        "h = Reshape((4, 4, 128))(h)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(128, 4, strides=(2, 2), padding='same')(h) # Shape (8, 8, 128)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(64, 4, strides=(2, 2), padding='same')(h) # Shape (16, 16, 64)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(32, 4, strides=(2, 2), padding='same')(h) # Shape (32, 32, 32)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "x_decoded = Conv2DTranspose(1, 4, strides=(1, 1), padding='same', activation='sigmoid')(h) # Shape (32, 32, 1)\n",
        "\n",
        "decoder = Model(z_in, x_decoded)\n",
        "\n",
        "# VAE\n",
        "x_recon = decoder(z)\n",
        "\n",
        "vae = Model(x, x_recon)\n",
        "\n",
        "# Compile model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
        "steps_per_epoch = 54000 / batch_size\n",
        "lr_schedule = PiecewiseConstantDecay([steps_per_epoch * halve_at_epoch], [initial_lr, initial_lr/2])\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "vae.compile(optimizer=optimizer, loss=vae_loss(z_mean, z_log_var), metrics=[recon, KL(z_mean, z_log_var)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8inwIbj1BMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_hist = []\n",
        "if TRAIN:\n",
        "    for i in range(15):\n",
        "        # Fit model\n",
        "        vae.compile(optimizer=optimizer, loss=vae_loss(z_mean, z_log_var, beta), metrics=[recon, KL(z_mean, z_log_var)])\n",
        "        hist = vae.fit(x_train, x_train, batch_size=batch_size, epochs=10, verbose=1, validation_split=0.1, callbacks=[UpdateBeta()])\n",
        "\n",
        "        total_hist.append(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2RFcdyF1ax6",
        "colab_type": "text"
      },
      "source": [
        "# Save training history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuEU2W7zpAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if TRAIN:\n",
        "    loss = []\n",
        "    recon = []\n",
        "    kl = []\n",
        "    val_loss = []\n",
        "    val_recon = []\n",
        "    val_kl = []\n",
        "    for hist in total_hist:\n",
        "        loss += hist.history['loss']\n",
        "        recon += hist.history['recon']\n",
        "        kl += hist.history['kl']\n",
        "\n",
        "        val_loss += hist.history['val_loss']\n",
        "        val_recon += hist.history['val_recon']\n",
        "        val_kl += hist.history['val_kl']\n",
        "\n",
        "    loss = np.asarray(loss)\n",
        "    recon = np.asarray(recon)\n",
        "    kl = np.asarray(kl)\n",
        "    val_loss = np.asarray(val_loss)\n",
        "    val_recon = np.asarray(val_recon)\n",
        "    val_kl = np.asarray(val_kl)\n",
        "    from numpy import savetxt\n",
        "    # save history\n",
        "    data = np.asarray([loss, recon, kl, val_loss, val_recon, val_kl])\n",
        "    savetxt('naive_VAE_MNIST_learngamma.csv', data, delimiter=',')\n",
        "\n",
        "    vae.save_weights('naive_VAE_MNIST_learngamma.h5')\n",
        "else:\n",
        "    vae.load_weights('naive_VAE_MNIST_learngamma.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJySOZEnZ2nm",
        "colab_type": "text"
      },
      "source": [
        "# Generation and Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSJxlGJJXXgT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "ee98a301-c629-4be5-e167-5ed0c58f991a"
      },
      "source": [
        "# Reconstruction\n",
        "n = 10\n",
        "digit_size = input_dim[0]\n",
        "\n",
        "x_recon = vae.predict(x_test, batch_size=batch_size)\n",
        "x_recon = np.reshape(x_recon, (-1, digit_size, digit_size))\n",
        "x_test = np.reshape(x_test, (-1, digit_size, digit_size))\n",
        "figure = np.zeros((2 * digit_size, n * digit_size))\n",
        "\n",
        "for i in range(n):\n",
        "    figure[:digit_size, i * digit_size: (i+1) * digit_size] = x_test[i]\n",
        "    figure[digit_size:, i * digit_size: (i+1) * digit_size] = x_recon[i]\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, ) + input_dim)\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.savefig('naive_VAE_MNIST_learngamma_reconstruction.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9c956481582c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdigit_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3824\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3825\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3827\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP5znLTRYHmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation\n",
        "n = 10 #figure with n x n digits\n",
        "digit_size = 32\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "# we will sample n points randomly sampled\n",
        "\n",
        "z_sample = np.random.normal(size=(n**2, latent_dim), scale=1)\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        x_decoded = decoder.predict(np.array([z_sample[i + n * j]]))\n",
        "        digit = x_decoded.reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "            j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.savefig('naive_VAE_MNIST_learngamma_generation.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiYOdNEM49NC",
        "colab_type": "text"
      },
      "source": [
        "# Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_APs78O48l_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T = 10\n",
        "N = 15\n",
        "\n",
        "s = 10\n",
        "k = 100\n",
        "\n",
        "X_final = [0] * N\n",
        "for n in range(N):\n",
        "    x_a = x_test[n + s]\n",
        "    x_b = x_test[n + k]\n",
        "\n",
        "    z_a = encoder.predict(np.reshape(x_a, (1, 32, 32, 1)))[0]\n",
        "    z_b = encoder.predict(np.reshape(x_b, (1, 32, 32, 1)))[0]\n",
        "\n",
        "\n",
        "    X = [0] * T\n",
        "    for i in range(T):\n",
        "        t = i/T\n",
        "        z = t * z_a + (1 - t) * z_b\n",
        "\n",
        "        X[i] = decoder.predict(z)\n",
        "    \n",
        "    X_final[n] = X\n",
        "\n",
        "digit_size = 32\n",
        "figure = np.zeros((N * digit_size, T * digit_size))\n",
        "for n in range(N):\n",
        "    for i in range(T):\n",
        "        figure[n * digit_size : (n+1) * digit_size, i * digit_size : (i+1) * digit_size] = X_final[n][i][0, :, :, 0]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.savefig('naive_VAE_MNIST_learngamma_interpolation.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozb--VBHCYUB",
        "colab_type": "text"
      },
      "source": [
        "#Nearest Neighbour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKe4Yp3QCXv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to investigate overfitting\n",
        "def get_distance(x, y_vec):\n",
        "    digit_size = x.shape[0]\n",
        "    x = np.reshape(x, (digit_size**2, ))\n",
        "    y_vec = np.reshape(y_vec, (-1, digit_size**2, ))\n",
        "\n",
        "    res = np.zeros((y_vec.shape[0], ))\n",
        "    for j in range(y_vec.shape[0]):\n",
        "        res[j] = np.mean(np.square(x - y_vec[j]))\n",
        "\n",
        "    return res\n",
        "\n",
        "N = 3 # Images we need to check\n",
        "digit_size = 32\n",
        "z = np.random.normal(size=(N, latent_dim))\n",
        "x_gen = decoder.predict(z)\n",
        "figure = np.zeros((N * digit_size, 2 * digit_size))\n",
        "for i in range(N):\n",
        "    x = x_gen[i]\n",
        "    distances = get_distance(x, x_train)\n",
        "    m = np.min(distances)\n",
        "    i_m = np.argmin(distances)\n",
        "\n",
        "    figure[i * digit_size : (i+1) * digit_size, : digit_size] = x[:, :, 0]\n",
        "    figure[i * digit_size : (i+1) * digit_size, digit_size :] = x_train[i_m][:, :, 0]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.savefig('naive_VAE_MNIST_learngamma_NN.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8oXRskqZud1",
        "colab_type": "text"
      },
      "source": [
        "# Metrics Evaluation\n",
        "\n",
        "First of all, we want to evaluate the ability of the model of generate high quality samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX2Tcb4uYrDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "import tensorflow as tf\n",
        "#from keras.applications.inception_v3 import preprocess_input\n",
        "#from skimage.transform import resize\n",
        "#from tensorflow.keras.models import load_model\n",
        "#import os\n",
        "#from matplotlib import pyplot\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3), weights='imagenet')\n",
        "\n",
        "def get_inception_activations(inps, batch_size=100):\n",
        "    n_batches = inps.shape[0]//batch_size\n",
        "    act = np.zeros([inps.shape[0], 2048], dtype = np.float32)\n",
        "    for i in range(n_batches):\n",
        "        inp = inps[i * batch_size:(i + 1) * batch_size]\n",
        "        inpr = tf.image.resize(inp, (299, 299))\n",
        "        act[i * batch_size:(i + 1) * batch_size] = model.predict(inpr,steps=1)\n",
        "        \n",
        "        print('Processed ' + str((i+1) * batch_size) + ' images.')\n",
        "    return act\n",
        "\n",
        "def get_fid(images1, images2):\n",
        "    print(images1.shape)\n",
        "    print(images2.shape)\n",
        "    print(type(images1))\n",
        "    # calculate activations\n",
        "    act1 = get_inception_activations(images1,batch_size=100)\n",
        "    #print(np.shape(act1))\n",
        "    act2 = get_inception_activations(images2,batch_size=100)\n",
        "    # compute mean and covariance statistics\n",
        "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "    # calculate sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "    # compute sqrt of product between cov\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    # calculate score\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid\n",
        "\n",
        "z_sample = np.random.normal(0, 1, size=(x_test.shape[0], latent_dim))\n",
        "x_gen = decoder.predict(z_sample)\n",
        "x_gens = np.zeros((x_test.shape[0], x_test.shape[1], x_test.shape[2], 3))\n",
        "x_tests = np.zeros((x_test.shape[0], x_test.shape[1], x_test.shape[2], 3))\n",
        "for i in range(3):\n",
        "\tx_gens[:, :, :, i] = x_gen[:, :, :, 0]\n",
        "\tx_tests[:, :, :, i] = x_test[:, :, :, 0]\n",
        "\n",
        "fid = get_fid(x_tests, x_gens)\n",
        "print('\\n FID: %.3f' % fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MjpF7fqaL0I",
        "colab_type": "text"
      },
      "source": [
        "### Deactivated Latent Variables, Variance Loss and Variance Law\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RJr0e6baKUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_deactivated_variables(z_var, treshold = 0.8):\n",
        "    z_var = np.mean(z_var, axis=0)\n",
        "\n",
        "    return np.sum(z_var > treshold)\n",
        "\n",
        "def loss_variance(x_true, x_recon):\n",
        "    x_true = np.reshape(x_true, (-1, np.prod(x_true.shape[1:])))\n",
        "    x_recon = np.reshape(x_recon, (-1, np.prod(x_recon.shape[1:])))\n",
        "\n",
        "    var_true = np.mean(np.var(x_true, axis=1), axis=0)\n",
        "    var_recon = np.mean(np.var(x_recon, axis=1), axis=0)\n",
        "\n",
        "    return np.abs(var_true - var_recon)\n",
        "\n",
        "########################################################################################################################\n",
        "# SHOW THE RESULTS\n",
        "########################################################################################################################\n",
        "\n",
        "_, z_mean, z_log_var = encoder.predict(x_test, batch_size=batch_size)\n",
        "z_var = np.exp(z_log_var)\n",
        "n_deact = count_deactivated_variables(z_var)\n",
        "print('We have a total of ', latent_dim, ' latent variables. ', count_deactivated_variables(z_var), ' of them are deactivated')\n",
        "\n",
        "var_law = np.mean(np.var(z_mean, axis=0) + np.mean(z_var, axis=0))\n",
        "print('Variance law has a value of: ', var_law)\n",
        "\n",
        "x_recon = vae.predict(x_train, batch_size=batch_size)\n",
        "print('We lost ', loss_variance(x_test, x_recon), 'Variance of the original data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFzldtCQVN0D",
        "colab_type": "text"
      },
      "source": [
        "### Latent space matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd3IeC85VSXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to verify if q(z) = p(z).\n",
        "\n",
        "# Moments Matching\n",
        "# Generate samples from q(z) and for p(z)\n",
        "# p(z) = N(0, I)\n",
        "# q(z) = E_q(x)[q(z|x)]\n",
        "#\n",
        "# For every moment we compare the moments\n",
        "n = len(x_test)\n",
        "\n",
        "p_samples = np.random.normal(size=(n, latent_dim))\n",
        "q_samples = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "\n",
        "from scipy.stats import moment\n",
        "# First moment matching:\n",
        "p_first_moment = np.mean(moment(p_samples, moment=1, axis=0))\n",
        "q_first_moment = np.mean(moment(q_samples, moment=1, axis=0))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"First moment of p(z): \" + str(p_first_moment))\n",
        "print(\"First moment of q(z): \" + str(q_first_moment))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Second moment matching:\n",
        "p_second_moment = np.mean(moment(p_samples, moment=2, axis=0))\n",
        "q_second_moment = np.mean(moment(q_samples, moment=2, axis=0))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Second moment of p(z): \" + str(p_second_moment))\n",
        "print(\"Second moment of q(z): \" + str(q_second_moment))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Thid moment matching:\n",
        "p_third_moment = np.mean(moment(p_samples, moment=3, axis=0))\n",
        "q_third_moment = np.mean(moment(q_samples, moment=3, axis=0))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Third moment of p(z): \" + str(p_third_moment))\n",
        "print(\"Third moment of q(z): \" + str(q_third_moment))\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFA_pmVbO57K",
        "colab_type": "text"
      },
      "source": [
        "# MMD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1gS1ToWfLJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To verify that q(z) = p(z) we can also use MMD\n",
        "\n",
        "def compute_kernel(x, y):\n",
        "    return np.exp(-np.mean(np.square(x - y), axis=-1))\n",
        "\n",
        "def compute_mmd(x, y):\n",
        "    x_kernel = compute_kernel(x, x)\n",
        "    y_kernel = compute_kernel(y, y)\n",
        "    xy_kernel = compute_kernel(x, y)\n",
        "\n",
        "    return np.mean(x_kernel) + np.mean(y_kernel) - 2 * np.mean(xy_kernel) \n",
        "\n",
        "z_p = np.random.normal(size=(len(x_test), latent_dim)) # sample from p(z)\n",
        "z_q = encoder.predict(x_test, batch_size=batch_size)[0]  # sample from q(z)\n",
        "\n",
        "z_p = z_p.astype('float32')\n",
        "z_q = z_q.astype('float32')\n",
        "\n",
        "mmd = compute_mmd(z_p, z_q)\n",
        "print('MMD between p(z) and q(z) is ' + str(mmd))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}