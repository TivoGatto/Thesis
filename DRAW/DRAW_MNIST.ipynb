{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRAW MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvWzDFkg7JjMNQ/1piwTSm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TivoGatto/Thesis/blob/master/DRAW/DRAW_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJb7PL6zbmGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LIBRARIES\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Lambda, LSTM, Activation, Add, Reshape, Concatenate, Multiply, Flatten\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh_nuwSYbtSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PARAMETERS\n",
        "input_dim = 32 * 32\n",
        "intermediate_dim = 256\n",
        "latent_dim = 100\n",
        "\n",
        "T = 20 # Number of cycles\n",
        "N = 5  # Dimension of the attention window\n",
        "\n",
        "total_latent_dimension = latent_dim * T # Number of effective latent variables\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "\n",
        "# CONSTANT\n",
        "ATTENTION  = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mbeTX1Ob1EC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTIONS\n",
        "\n",
        "def draw_loss(x_true, x_pred):\n",
        "\t# Compute the loss for DRAW Model\n",
        "\tx_pred = sigm(x_pred) \n",
        "\n",
        "\txent_loss = keras.losses.binary_crossentropy(x_true, x_pred) # Reconstruction loss (MSE between original image x and reconstructed x_bar\n",
        "\n",
        "\t# D_{KL} Loss, summed over cycles. For q(z|x) = N(z_mean, z_var), p(z) = N(0, I) we have:\n",
        "\t# D{KL} = 1/2 * (z_mean^2 + z_var^2 - z_log_var - 1)\n",
        "\treg_loss = 0\n",
        "\tfor t in range(1, T+1):\n",
        "\t\treg_loss += 0.5 * K.sum(K.square(z_mean[t]) + K.exp(z_log_var[t]) - z_log_var[t], 1) - 0.5\n",
        "\n",
        "\treturn K.mean(xent_loss + reg_loss / total_latent_dimension)\n",
        "\n",
        "def Regularizer(x_true, x_pred):\n",
        "\t# I use this function as metric to check the behaviour of Regularizer D_{KL}(q(z|x) || p(z)) during training\n",
        "\treg_loss = 0\n",
        "\n",
        "\tfor t in range(1, T+1):\n",
        "\t\treg_loss += 0.5 * K.sum(K.square(z_mean[t]) + K.exp(z_log_var[t]) - z_log_var[t], axis=-1) - 0.5\n",
        "\n",
        "\treturn K.mean(reg_loss) / total_latent_dimension\n",
        " \n",
        "def Reconstruction(x_true, x_pred):\n",
        "\tx_pred = sigm(x_pred) \n",
        "\t# I use this function as metric to check the behaviour of Reconstruction loss during training\n",
        "\treturn keras.losses.binary_crossentropy(x_true, x_pred)\n",
        "\n",
        "def Conc(args):\n",
        "\t# Concatenate a list of layers.\n",
        "\treturn K.concatenate(args, axis=-1)\n",
        "\n",
        "def Sampling(args):\n",
        "\t# Sample z from q(z|x) = N(z_mean, z_var) with reparameterization trick.\n",
        "\t# eps <- N(O, I)\n",
        "\t# z = eps * z_var + z_mean\n",
        "\tz_mean, z_log_var = args\n",
        "\tepsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
        "\n",
        "\treturn z_mean + K.exp(0.5 * z_log_var) * epsilon \n",
        "\n",
        "def Read(args):\n",
        "\t# Read with no attention. Just concatenate [x, x_hat]\n",
        "    x, x_hat, h_dec = args\n",
        "\n",
        "    if ATTENTION:\n",
        "        pass\n",
        "    else:\n",
        "        return conc([x, x_hat])\n",
        "\n",
        "def Write(args):\n",
        "\t# Write with no attention. Just a NN with linear activation function to \"reshape\" h_dec in input_dim.\n",
        "    h_dec = args\n",
        "    if ATTENTION:\n",
        "        pass\n",
        "\n",
        "def Add_time(args):\n",
        "\t# Add an additional dimension to a layer. This is needed for LSTM.\n",
        "\treturn K.expand_dims(args, 1)\n",
        " \n",
        "def sigmoid(x): \n",
        "\t# Compute sigm(x) = 1 / 1 + exp(-x)\n",
        "\treturn 1 / (1 + np.exp(-x))\n",
        " \n",
        "def pad(x, d):\n",
        "    size = x.shape[0]\n",
        "    h, w = x.shape[1:]\n",
        "\n",
        "    x = np.reshape(x, (size, h, w, 1))\n",
        "\n",
        "    x_padded = np.zeros(shape=(size, ) + d)\n",
        "    x_padded[:, :h, :w] = x\n",
        "\n",
        "    return x_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVfb4NtLcmmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b448d0ec-fa9f-4c63-be38-710bdfd8b7f2"
      },
      "source": [
        "# IMPORT DATA\n",
        "\"\"\"\n",
        "Import MNIST and preprocess it to have [0, 1] values.\n",
        "\"\"\"\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = pad(x_train, (32, 32, 1)) / 255 # For MNIST, we pad x_train and x_test in \n",
        "x_test  = pad(x_test, (32, 32, 1)) / 255 # shape (32, 32, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = np.reshape(x_train, (-1, input_dim))\n",
        "x_test = np.reshape(x_test, (-1, input_dim))\n",
        "\n",
        "print('x_train shape: ' + str(x_train.shape))\n",
        "print('x_test shape: ' + str(x_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 1024)\n",
            "x_test shape: (10000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBpWXxQ7cPnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL\n",
        "\n",
        "# Convert previous function in Lambda layer to be used in the Model.\n",
        "sigm = Activation(\"sigmoid\")\n",
        "conc = Lambda(Conc)\n",
        "sampling = Lambda(Sampling)\n",
        "add_time = Lambda(Add_time)\n",
        "\n",
        "if ATTENTION:\n",
        "    read = Lambda(Read)\n",
        "    write = Lambda(Write)\n",
        "else:\n",
        "    read = Lambda(Read)\n",
        "    write = Dense(input_dim)\n",
        "\n",
        "\n",
        "# Initialize Encoder and Decoder as LSTM\n",
        "encoder = LSTM(intermediate_dim, stateful=False, return_state=True, name='Encoder')\n",
        "decoder = LSTM(intermediate_dim, stateful=False, return_state=True, name='Decoder')\n",
        "\n",
        "\n",
        "# Initialize a T-length list where we can store the values of the feature during cycles\n",
        "x_hat = [0] * (T + 1)\n",
        "r     = [0] * (T + 1)\n",
        "z     = [0] * (T + 1)\n",
        "\n",
        "z_mean = [0] * (T + 1)\n",
        "z_log_var = [0] * (T + 1)\n",
        "\n",
        "h_enc = [0] * (T + 1)\n",
        "h_dec = [0] * (T + 1)\n",
        "C     = [0] * (T + 1)\n",
        "\n",
        "\n",
        "c_enc = [0] * (T + 1)\n",
        "c_dec = [0] * (T + 1)\n",
        "\n",
        "c_enc_init = Input(tensor=K.zeros(shape=(batch_size,  intermediate_dim)), name='c_enc')\n",
        "c_dec_init = Input(tensor=K.zeros(shape=(batch_size,  intermediate_dim)), name='c_dec')\n",
        "\n",
        "c_enc[0] = c_enc_init\n",
        "c_dec[0] = c_dec_init\n",
        "\n",
        "# Inizialize Input layers\n",
        "x = Input(shape=(input_dim, ), batch_shape=(batch_size, input_dim), name='Input_img')\n",
        "h_enc_init = Input(tensor=K.zeros(shape=(batch_size,  intermediate_dim)), name='h_enc')\n",
        "h_dec_init = Input(tensor=K.zeros(shape=(batch_size,  intermediate_dim)), name='h_dec')\n",
        "C_0 = Input(tensor=K.zeros(shape=(batch_size, input_dim)), name='C_input')\n",
        "\n",
        "# And assign them to the first element of our list\n",
        "h_enc[0] = h_enc_init\n",
        "h_dec[0] = h_dec_init\n",
        "C[0]     = Dense(input_dim, name='C_0')(C_0) # C[0] = C_0 (we used a Dense layer which do nothing to coverge C_0 into a Dense Layer)\n",
        "\n",
        "for t in range(1, T+1):\n",
        "    x_hat[t] = keras.layers.Subtract(name='x_hat_'+str(t))([x, sigm(C[t-1])]) # Error Image\n",
        "    r[t]     = read([x, x_hat[t], h_dec[t-1]])            # Information about x, x_hat, h_dec[t-1]\n",
        "\n",
        "    h_enc[t], _, c_enc[t] = encoder(add_time(conc([r[t], h_dec[t-1]])), initial_state=[h_enc[t-1], c_enc[t-1]]) # Encoded x\n",
        "\n",
        "    z_mean[t] = Dense(latent_dim, name='z_mean_'+str(t))(h_enc[t]) \n",
        "    z_log_var[t] = Dense(latent_dim, name='z_log_var_'+str(t))(h_enc[t])\n",
        "\n",
        "    z[t] = sampling([z_mean[t], z_log_var[t]])\n",
        "\n",
        "    h_dec[t], _, c_dec[t] = decoder(add_time(z[t]), initial_state=[h_dec[t-1], c_dec[t-1]]) # Decoded z\n",
        "\n",
        "    C[t] = Add(name='C_'+str(t))([C[t-1], write(h_dec[t])]) # Update the canvas\n",
        "\n",
        "vae = Model([x, C_0, h_dec_init, h_enc_init, c_enc_init, c_dec_init], C[T])\n",
        "\n",
        "optmizers = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5)\n",
        "vae.compile(optimizer=optmizers, loss=draw_loss, metrics=[Reconstruction, Regularizer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjDdRzG8cpyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit model\n",
        "hist = vae.fit(x_train, x_train, batch_size=batch_size, epochs=50, verbose=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgJygDI1dAE9",
        "colab_type": "text"
      },
      "source": [
        "# Generation and Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7zwLoirc_aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST RECONSTRUCTION\n",
        "x_recon = vae.predict(x_train, batch_size=batch_size) # Reconstruct MNIST Digit from x_train\n",
        "x_recon = sigmoid(x_recon) # Apply sigmoid = 1 / 1 + exp(-x) to reconstructed canvas.\n",
        "\n",
        "digit_size = 32 # Size of the digit. It is supposed to be a digit_size x digit_size image\n",
        "n = 5 # Number of showed images\n",
        "\n",
        "figure = np.zeros(shape=(digit_size * n, digit_size * 2))\n",
        "for i in range(n):\n",
        "    X_true  = x_train[i]\n",
        "    X_true  = np.reshape(X_true, (digit_size, digit_size))\n",
        "\n",
        "    X_recon = x_recon[i]\n",
        "    X_recon = np.reshape(X_recon, (digit_size, digit_size))\n",
        "\n",
        "    figure[i * digit_size : (i + 1) * digit_size, : digit_size] = X_true\n",
        "    figure[i * digit_size : (i + 1) * digit_size, digit_size :] = X_recon\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2g8JNXMdMl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RE-DEFINE MODELS NEEDED FOR GENERATION \n",
        "WEIGHTS = vae.layers[16].get_weights() # Save Weights of LSTM Decoder\n",
        "LSTM_decoder = LSTM(intermediate_dim, stateful=False, return_state=True)    # Instantiate another copy of LSTM Layer\n",
        "\n",
        "# DEFINE DECODER MODEL\n",
        "z_in = Input(shape=(latent_dim), batch_shape=(batch_size, latent_dim))\n",
        "h_in = Input(shape=(intermediate_dim), batch_shape=(batch_size, intermediate_dim))\n",
        "c_in = Input(shape=(intermediate_dim), batch_shape=(batch_size, intermediate_dim))\n",
        "\n",
        "h_out, _, c_out = LSTM_decoder(add_time(z_in), initial_state=[h_in, c_in])\n",
        "DECODER = Model([z_in, h_in, c_in], h_out)\n",
        "\n",
        "# Load WEIGHTS into the new Model\n",
        "DECODER.layers[-1].set_weights(WEIGHTS)\n",
        "\n",
        "# DEFINE WRITE MODEL\n",
        "h_in = Input(shape=(intermediate_dim, ), batch_shape=(batch_size, intermediate_dim))\n",
        "x_out = Dense(input_dim)(h_in)\n",
        "WRITE = Model(h_in, x_out)\n",
        "\n",
        "WRITE.layers[-1].set_weights(vae.layers[17].get_weights()) # Load write models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4oOSkt3dPbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GENERATE NEW IMAGES WITH PRIOR p(z) = N(0, I)\n",
        "h_dec_gen = np.zeros(shape=(batch_size, intermediate_dim)) # h_dec_0 for generation\n",
        "c_dec_gen = np.zeros(shape=(batch_size, intermediate_dim))\n",
        "\n",
        "C = [0] * (T + 1)                  # Canvas\n",
        "C[0] = np.zeros(shape=(input_dim)) # Initialize Canvas with a zero-valued images\n",
        "for t in range(1, T + 1):\n",
        "    Z = np.random.normal(size=(batch_size, latent_dim))\n",
        "    h_dec_gen = DECODER.predict([Z, h_dec_gen, c_dec_gen], batch_size=batch_size)\n",
        "\n",
        "    C[t] = C[t-1] + WRITE.predict(h_dec_gen)\n",
        "\n",
        "\"\"\"\n",
        "Now we have a tensor C of dimension (T, batch_size, input_dim) where:\n",
        "    C[t] is the canvas at time t, t = 1, ..., T + 1\n",
        "    C[t] is a matrix of dimension (batch_size, input_dim) for each t such that\n",
        "         it contains a batch_size number of generated images at time t\n",
        "\n",
        "    We want to show these results.\n",
        "\"\"\"\n",
        "\n",
        "digit_size = 32\n",
        "n = 10\n",
        "\n",
        "figure = np.zeros((digit_size * n, digit_size * (T + 1)))\n",
        "for t in range(1, T + 1):\n",
        "    C[t] = sigmoid(C[t])\n",
        "    C[t] = np.reshape(C[t], (-1, digit_size, digit_size))\n",
        "\n",
        "    for i in range(n):\n",
        "        figure[i * digit_size : (i+1) * digit_size, t * digit_size : (t+1)*digit_size] = C[t][i]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atQXZg7zfx5w",
        "colab_type": "text"
      },
      "source": [
        "# Metrics Evaluation\n",
        "\n",
        "First of all, we want to evaluate the ability of the model of generate high quality samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWkq0shcf2XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FID Score\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "from skimage.transform import resize\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "# Functions needed to compute FID score\n",
        "def scale_images(images, new_shape): # Scale an image in a new shape using NN Interpolation\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn np.asarray(images_list)\n",
        "\n",
        "\n",
        "def calculate_fid(model, images1, images2): # Calculate Frechet Inception Distance between images1, images2\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "\n",
        "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "\tif np.iscomplexobj(covmean): # Check if the sqrtm is complex\n",
        "\t\tcovmean = covmean.real\n",
        "\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "z_sample = np.random.normal(0, 1, size=(sample_size, latent_dim))\n",
        "sample = np.random.randint(0, len(x_test), size=sample_size)\n",
        "x_gen = decoder.predict(z_sample)\n",
        "x_real = x_test[sample]\n",
        "\n",
        "x_gen = evaluate.scale_images(x_gen, (299, 299, 1))\n",
        "x_real = evaluate.scale_images(x_real, (299, 299, 1))\n",
        "print('Scaled', x_gen.shape, x_real.shape)\n",
        "\n",
        "x_gen_t = preprocess_input(x_gen)\n",
        "x_real_t = preprocess_input(x_real)\n",
        "\n",
        "x_gen = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "x_real = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "for i in range(3):\n",
        "    x_gen[:, :, :, i] = x_gen_t[:, :, :, 0]\n",
        "    x_real[:, :, :, i] = x_real_t[:, :, :, 0]\n",
        "print('Final', x_gen.shape, x_real.shape)\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "\n",
        "# fid between images1 and images2\n",
        "fid = evaluate.calculate_fid(model, x_real, x_gen)\n",
        "print('FID (different): %.3f' % fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nASOp828f6Gj",
        "colab_type": "text"
      },
      "source": [
        "### Deactivated Latent Variables, Variance Loss and Variance Law\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IXXAWGCf4a2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_deactivated_variables(z_var, treshold = 0.8):\n",
        "    z_var = np.mean(z_var, axis=0)\n",
        "\n",
        "    return np.sum(z_var > treshold)\n",
        "\n",
        "def loss_variance(x_true, x_recon):\n",
        "    x_true = np.reshape(x_true, (-1, np.prod(x_true.shape[1:])))\n",
        "    x_recon = np.reshape(x_recon, (-1, np.prod(x_recon.shape[1:])))\n",
        "\n",
        "    var_true = np.mean(np.var(x_true, axis=1), axis=0)\n",
        "    var_recon = np.mean(np.var(x_recon, axis=1), axis=0)\n",
        "\n",
        "    return np.abs(var_true - var_recon)\n",
        "\n",
        "########################################################################################################################\n",
        "# SHOW THE RESULTS\n",
        "########################################################################################################################\n",
        "\n",
        "_, z_mean, z_log_var = encoder.predict(x_test, batch_size=batch_size)\n",
        "z_var = np.exp(z_log_var)\n",
        "n_deact = count_deactivated_variables(z_var)\n",
        "print('We have a total of ', latent_dim, ' latent variables. ', count_deactivated_variables(z_var), ' of them are deactivated')\n",
        "\n",
        "var_law = np.mean(np.var(z_mean, axis=0) + np.mean(z_var, axis=0))\n",
        "print('Variance law has a value of: ', var_law)\n",
        "\n",
        "x_recon = vae.predict(x_train, batch_size=batch_size)\n",
        "print('We lost ', loss_variance(x_test, x_recon), 'Variance of the original data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KpYY4dHf8be",
        "colab_type": "text"
      },
      "source": [
        "### Latent space matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7qHgZ7nf-BE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to verify if q(z) = p(z).\n",
        "\n",
        "# Moments Matching\n",
        "# Generate samples from q(z) and for p(z)\n",
        "# p(z) = N(0, I)\n",
        "# q(z) = E_q(x)[q(z|x)]\n",
        "#\n",
        "# For every moment we compare the log-moments\n",
        "n = len(x_test)\n",
        "\n",
        "p_samples = np.random.normal(size=(n, latent_dim))\n",
        "q_samples = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "\n",
        "from scipy.stats import moment\n",
        "# First moment matching:\n",
        "p_first_moment = np.log(np.mean(moment(p_samples, moment=1, axis=0)))\n",
        "q_first_moment = np.log(np.mean(moment(q_samples, moment=1, axis=0)))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"First log-moment of p(z): \" + str(p_first_moment))\n",
        "print(\"First log-moment of q(z): \" + str(q_first_moment))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Second moment matching:\n",
        "p_second_moment = np.log(np.mean(moment(p_samples, moment=2, axis=0)))\n",
        "q_second_moment = np.log(np.mean(moment(q_samples, moment=2, axis=0)))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Second log-moment of p(z): \" + str(p_second_moment))\n",
        "print(\"Second log-moment of q(z): \" + str(q_second_moment))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Thid moment matching:\n",
        "p_third_moment = np.log(np.mean(moment(p_samples, moment=3, axis=0)))\n",
        "q_third_moment = np.log(np.mean(moment(q_samples, moment=3, axis=0)))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Third log-moment of p(z): \" + str(p_third_moment))\n",
        "print(\"Third log-moment of q(z): \" + str(q_third_moment))\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}