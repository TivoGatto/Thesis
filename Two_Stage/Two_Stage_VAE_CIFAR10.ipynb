{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Two-Stage VAE CIFAR10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPu+jei3RvZ5QNA6F9y1S6t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TivoGatto/Thesis/blob/master/Two_Stage/Two_Stage_VAE_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Woes7QwzwLkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LIBRARIES\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, ReLU, Dense, Flatten, Reshape, Conv2DTranspose, Lambda\n",
        "from keras.datasets import cifar10\n",
        "import keras.backend as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrJYB3STwT8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "input_dim = (32, 32, 3)\n",
        "latent_dim = 16\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 100\n",
        "\n",
        "beta = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHpLYiSewWad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions\n",
        "def vae_loss(z_mean, z_log_var):\n",
        "    def loss(x_true, x_pred):\n",
        "        x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "        x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "        L_rec = 0.5 * K.sum(K.square(x_true - x_pred), axis=-1)\n",
        "        L_KL = 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1)\n",
        "\n",
        "        return K.mean(L_rec + beta * L_KL)\n",
        "    return loss\n",
        "\n",
        "def recon(x_true, x_pred):\n",
        "    x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "    x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "    return K.mean(0.5 * K.sum(K.square(x_true - x_pred), axis=-1))\n",
        "\n",
        "def KL(z_mean, z_log_var):\n",
        "    def kl(x_true, x_pred):\n",
        "        return K.mean(0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1))\n",
        "    return kl\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    eps = K.random_normal(shape=(100, latent_dim)) # 100 = batch_size\n",
        "\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * eps\n",
        "\n",
        "def pad(x, d):\n",
        "    size = x.shape[0]\n",
        "    h, w = x.shape[1:]\n",
        "\n",
        "    x = np.reshape(x, (size, h, w, 1))\n",
        "\n",
        "    x_padded = np.zeros(shape=(size, ) + d)\n",
        "    x_padded[:, :h, :w] = x\n",
        "\n",
        "    return x_padded"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5yOCdKHwW_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "\n",
        "print('x_train shape: ' + str(x_train.shape))\n",
        "print('x_test shape: ' + str(x_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J3gw1yEwYXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First stage VAE\n",
        "\n",
        "# Model Architecture\n",
        "# ENCODER\n",
        "x = Input(shape=input_dim) # Shape (32, 32, 1)\n",
        "\n",
        "h = Conv2D(128, 4, strides=(2, 2), padding='same')(x) # Shape (16, 16, 128)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(256, 4, strides=(2, 2), padding='same')(h) # Shape (8, 8, 256)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(512, 4, strides=(2, 2), padding='same')(h) # Shape (4, 4, 512)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(1024, 4, strides=(2, 2), padding='same')(h) # Shape (2, 2, 1024)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Flatten()(h)\n",
        "\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(x, [z, z_mean, z_log_var])\n",
        "\n",
        "# DECODER\n",
        "z_in = Input(shape=(latent_dim, ))\n",
        "\n",
        "h = Dense(8 * 8 * 1024)(z_in)\n",
        "h = Reshape((8, 8, 1024))(h)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(512, 4, strides=(2, 2), padding='same')(h) # Shape (16, 16, 512)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(256, 4, strides=(2, 2), padding='same')(h)\n",
        "#h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "x_decoded = Conv2DTranspose(3, 4, strides=(1, 1), padding='same', activation='sigmoid')(h)\n",
        "\n",
        "decoder = Model(z_in, x_decoded)\n",
        "\n",
        "# VAE\n",
        "x_recon = decoder(z)\n",
        "\n",
        "vae = Model(x, x_recon)\n",
        "\n",
        "# Compile model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(lr=0.001)\n",
        "\n",
        "vae.compile(optimizer=optimizer, loss=vae_loss(z_mean, z_log_var), metrics=[recon, KL(z_mean, z_log_var)])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M26wlEXxwZxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit model\n",
        "hist = vae.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVEZ4e6zw9ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Second Stage VAE\n",
        "\n",
        "# Loss\n",
        "def second_stage_loss(u_mean, u_log_var):\n",
        "    def loss(x_true, x_pred):\n",
        "        L_rec = 0.5 * K.sum(K.square(x_true - x_pred), axis=-1)\n",
        "        L_KL = 0.5 * K.sum(K.square(u_mean) + K.exp(u_log_var) - 1 - u_log_var, axis=-1)\n",
        "\n",
        "        return K.mean(L_rec + L_KL)\n",
        "    return loss\n",
        "\n",
        "def second_stage_sampling(args):\n",
        "    u_mean, u_log_var = args\n",
        "    eps = K.random_normal(shape=(batch_size, latent_dim))\n",
        "\n",
        "    return u_mean + eps * K.exp(0.5 * u_log_var)\n",
        "\n",
        "# Encoder\n",
        "intermediate_dim = 256\n",
        "\n",
        "z = Input(shape=(latent_dim, ))\n",
        "\n",
        "h = Dense(intermediate_dim, activation='relu')(z)\n",
        "h = Dense(intermediate_dim, activation='relu')(h)\n",
        "h = Dense(intermediate_dim, activation='relu')(h)\n",
        "\n",
        "u_mean = Dense(latent_dim)(h)\n",
        "u_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "u = Lambda(second_stage_sampling)([u_mean, u_log_var])\n",
        "second_encoder = Model(z, [u, u_mean, u_log_var])\n",
        "\n",
        "# Decoder\n",
        "u_in = Input(shape=(latent_dim, ))\n",
        "\n",
        "h = Dense(intermediate_dim, activation='relu')(u_in)\n",
        "h = Dense(intermediate_dim, activation='relu')(h)\n",
        "h = Dense(intermediate_dim, activation='relu')(h)\n",
        "\n",
        "z_decoded = Dense(latent_dim)(h)\n",
        "second_decoder = Model(u_in, z_decoded)\n",
        "\n",
        "# VAE\n",
        "z_reconstructed = second_decoder(u)\n",
        "second_vae = Model(z, z_reconstructed)\n",
        "second_vae.compile(optimizer=optimizer, loss=second_stage_loss(u_mean, u_log_var))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kIP78YiyVNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate second stage Dataset\n",
        "z_train = encoder.predict(x_train, batch_size=batch_size)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xt4tI9MIZ7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit second stage VAE\n",
        "second_vae.fit(z_train, z_train, batch_size=batch_size, epochs=epochs, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VZzFz_O00v5",
        "colab_type": "text"
      },
      "source": [
        "# Generation and Reconstruction\n",
        "\n",
        "Now we have two VAE network: \\\\\n",
        "1) u <- p(u) = N(0, I) \\\\\n",
        "2) z <- q(z) = E_p(u)[q(z|u)] \\\\\n",
        "3) x <- p(x) = E_q(z)[p(x|z)] \\\\\n",
        "\n",
        "And we can use them to generate new samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6IEv-P81YLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reconstruction\n",
        "\n",
        "\"\"\"\n",
        "Here we don't need to use the second stage VAE, 'cause we don't use q(z)\n",
        "\"\"\"\n",
        "n = 10\n",
        "digit_size = input_dim[0]\n",
        "\n",
        "x_recon = vae.predict(x_test, batch_size=batch_size)\n",
        "x_recon = np.reshape(x_recon, (-1, digit_size, digit_size))\n",
        "x_test = np.reshape(x_test, (-1, digit_size, digit_size))\n",
        "figure = np.zeros((2 * digit_size, n * digit_size))\n",
        "\n",
        "for i in range(n):\n",
        "    sample = np.random.randint(0, len(x_recon))\n",
        "    figure[:digit_size, i * digit_size: (i+1) * digit_size] = x_test[sample]\n",
        "    figure[digit_size:, i * digit_size: (i+1) * digit_size] = x_recon[sample]\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, ) + input_dim)\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFRRvQNm1bQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation\n",
        "n = 10 #figure with n x n digits\n",
        "digit_size = 32\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "# we will sample n points randomly sampled\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We want to sample z from q(z) = E_p(u)[q(z|u)]\n",
        "p(u) = N(0, I)\n",
        "\"\"\"\n",
        "u_sample = np.random.normal(size=(n**2, latent_dim), scale=1)\n",
        "z_sample = second_decoder.predict(u_sample, batch_size=batch_size)\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        x_decoded = decoder.predict(np.array([z_sample[i + n * j]]))\n",
        "        digit = x_decoded.reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "            j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt8OjcTE18xP",
        "colab_type": "text"
      },
      "source": [
        "# Metrics Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-DmaROH1-0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FID Score\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "from skimage.transform import resize\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "# Functions needed to compute FID score\n",
        "def scale_images(images, new_shape): # Scale an image in a new shape using NN Interpolation\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn np.asarray(images_list)\n",
        "\n",
        "\n",
        "def calculate_fid(model, images1, images2): # Calculate Frechet Inception Distance between images1, images2\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "\n",
        "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "\tif np.iscomplexobj(covmean): # Check if the sqrtm is complex\n",
        "\t\tcovmean = covmean.real\n",
        "\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "u_sample = np.random.normal(0, 1, size=(sample_size, latent_dim))\n",
        "z_sample = second_decoder.predict(u_sample, batch_size=batch_size)\n",
        "sample = np.random.randint(0, len(x_test), size=sample_size)\n",
        "x_gen = decoder.predict(z_sample)\n",
        "x_real = x_test[sample]\n",
        "\n",
        "x_gen = evaluate.scale_images(x_gen, (299, 299, 1))\n",
        "x_real = evaluate.scale_images(x_real, (299, 299, 1))\n",
        "print('Scaled', x_gen.shape, x_real.shape)\n",
        "\n",
        "x_gen_t = preprocess_input(x_gen)\n",
        "x_real_t = preprocess_input(x_real)\n",
        "\n",
        "x_gen = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "x_real = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "for i in range(3):\n",
        "    x_gen[:, :, :, i] = x_gen_t[:, :, :, 0]\n",
        "    x_real[:, :, :, i] = x_real_t[:, :, :, 0]\n",
        "print('Final', x_gen.shape, x_real.shape)\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "\n",
        "# fid between images1 and images2\n",
        "fid = evaluate.calculate_fid(model, x_real, x_gen)\n",
        "print('FID (different): %.3f' % fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny2v1XDY2F4w",
        "colab_type": "text"
      },
      "source": [
        "### Deactivated Latent Variables, Variance Loss and Variance Law\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsQxQALx2D8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_deactivated_variables(z_var, treshold = 0.8):\n",
        "    z_var = np.mean(z_var, axis=0)\n",
        "\n",
        "    return np.sum(z_var > treshold)\n",
        "\n",
        "def loss_variance(x_true, x_recon):\n",
        "    x_true = np.reshape(x_true, (-1, np.prod(x_true.shape[1:])))\n",
        "    x_recon = np.reshape(x_recon, (-1, np.prod(x_recon.shape[1:])))\n",
        "\n",
        "    var_true = np.mean(np.var(x_true, axis=1), axis=0)\n",
        "    var_recon = np.mean(np.var(x_recon, axis=1), axis=0)\n",
        "\n",
        "    return np.abs(var_true - var_recon)\n",
        "\n",
        "########################################################################################################################\n",
        "# SHOW THE RESULTS\n",
        "########################################################################################################################\n",
        "\n",
        "_, z_mean, z_log_var = encoder.predict(x_test, batch_size=batch_size)\n",
        "z_var = np.exp(z_log_var)\n",
        "n_deact = count_deactivated_variables(z_var)\n",
        "print('We have a total of ', latent_dim, ' latent variables. ', count_deactivated_variables(z_var), ' of them are deactivated')\n",
        "\n",
        "var_law = np.mean(np.var(z_mean, axis=0) + np.mean(z_var, axis=0))\n",
        "print('Variance law has a value of: ', var_law)\n",
        "\n",
        "x_recon = vae.predict(x_train, batch_size=batch_size)\n",
        "print('We lost ', loss_variance(x_test, x_recon), 'Variance of the original data')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}