{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RAE MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMK5ub6R6NZc1UkRKe0TpQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TivoGatto/Thesis/blob/master/RAE/RAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCze978NHxjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cce6c95d-3f2c-4e4a-f5a5-71d3c063e5fe"
      },
      "source": [
        "# LIBRARIES\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, ReLU, Dense, Flatten, Reshape, Conv2DTranspose, Lambda\n",
        "from keras.datasets import mnist\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipj-08OnH6GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "input_dim = (32, 32, 1)\n",
        "latent_dim = 16\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "lamb = 0.05\n",
        "beta = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPPCzklUH9LP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions\n",
        "def rae_loss(z):\n",
        "    def loss(x_true, x_pred):\n",
        "        x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "        x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "        L_rec = 0.5 * K.sum(K.square(x_true - x_pred), axis=-1)\n",
        "        L_rae = 0.5 * K.sum(K.square(z), axis=-1)\n",
        "\n",
        "        return K.mean(L_rec + beta * L_rae)\n",
        "    return loss\n",
        "\n",
        "def recon(x_true, x_pred):\n",
        "    x_true = K.reshape(x_true, (-1, np.prod(input_dim)))\n",
        "    x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))\n",
        "\n",
        "    return K.mean(0.5 * K.sum(K.square(x_true - x_pred), axis=-1))\n",
        "\n",
        "def RAE(z):\n",
        "    def rae(x_true, x_pred):\n",
        "        return K.mean(0.5 * K.sum(K.square(z), axis=-1))\n",
        "    return rae\n",
        "\n",
        "def pad(x, d):\n",
        "    size = x.shape[0]\n",
        "    h, w = x.shape[1:]\n",
        "\n",
        "    x = np.reshape(x, (size, h, w, 1))\n",
        "\n",
        "    x_padded = np.zeros(shape=(size, ) + d)\n",
        "    x_padded[:, :h, :w] = x\n",
        "\n",
        "    return x_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9oQGHF3IVZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = pad(x_train, input_dim) / 255 # For MNIST, we pad x_train and x_test in \n",
        "x_test  = pad(x_test, input_dim) / 255 # shape (32, 32, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "\n",
        "print('x_train shape: ' + str(x_train.shape))\n",
        "print('x_test shape: ' + str(x_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9spYpU7IX5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Architecture\n",
        "# ENCODER\n",
        "x = Input(shape=input_dim) # Shape (32, 32, 1)\n",
        "\n",
        "h = Conv2D(128, 4, strides=(2, 2), padding='same')(x) # Shape (16, 16, 128)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(256, 4, strides=(2, 2), padding='same')(h) # Shape (8, 8, 256)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(512, 4, strides=(2, 2), padding='same')(h) # Shape (4, 4, 512)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2D(1024, 4, strides=(2, 2), padding='same')(h) # Shape (2, 2, 1024)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Flatten()(h)\n",
        "\n",
        "z = Dense(latent_dim)(h)\n",
        "\n",
        "encoder = Model(x, z)\n",
        "\n",
        "# DECODER\n",
        "z_in = Input(shape=(latent_dim, ))\n",
        "\n",
        "h = Dense(8 * 8 * 1024, kernel_regularizer=l2(lamb))(z_in)\n",
        "h = Reshape((8, 8, 1024))(h)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(512, 4, strides=(2, 2), padding='same', kernel_regularizer=l2(lamb))(h) # Shape (16, 16, 512)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "h = Conv2DTranspose(256, 4, strides=(2, 2), padding='same', kernel_regularizer=l2(lamb))(h)\n",
        "h = BatchNormalization()(h)\n",
        "h = ReLU()(h)\n",
        "\n",
        "x_decoded = Conv2DTranspose(1, 4, strides=(1, 1), padding='same', kernel_regularizer=l2(lamb))(h)\n",
        "\n",
        "decoder = Model(z_in, x_decoded)\n",
        "\n",
        "# VAE\n",
        "x_recon = decoder(z)\n",
        "\n",
        "vae = Model(x, x_recon)\n",
        "\n",
        "# Compile the model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(lr=0.001)\n",
        "\n",
        "vae.compile(optimizer=optimizer, loss=rae_loss(z), metrics=[recon, RAE(z)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-i9agcGIZiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit model\n",
        "hist = vae.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zijO-BO5Tgc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Learn latent space distribution\n",
        "z_train = encoder.predict(x_train)\n",
        "\n",
        "prior_for_qz = \"Gaussian\" # Choose between GMM or Gaussian\n",
        "if prior_for_qz == \"GMM\":\n",
        "    from sklearn.mixture import GaussianMixture\n",
        "\n",
        "    z_density = GaussianMixture(n_components=10, max_iter=100)\n",
        "    z_density.fit(z_train)\n",
        "\n",
        "    print(\"Learned Gaussian\")\n",
        "elif prior_for_qz == \"Gaussian\":\n",
        "    from scipy.stats import norm\n",
        "\n",
        "    mean, std = norm.fit(z_train) # z_train is fitted to a gaussian N(mean, std)\n",
        "    print(\"Learned GMM\")\n",
        "else:\n",
        "    print(\"Distribution not found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hiEenogl6yo",
        "colab_type": "text"
      },
      "source": [
        "# Generation and Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dw1rtzLl-Kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reconstruction\n",
        "n = 10\n",
        "digit_size = input_dim[0]\n",
        "\n",
        "x_recon = vae.predict(x_test, batch_size=batch_size)\n",
        "x_recon = np.reshape(x_recon, (-1, digit_size, digit_size))\n",
        "x_test = np.reshape(x_test, (-1, digit_size, digit_size))\n",
        "figure = np.zeros((2 * digit_size, n * digit_size))\n",
        "\n",
        "for i in range(n):\n",
        "    sample = np.random.randint(0, len(x_recon))\n",
        "    figure[:digit_size, i * digit_size: (i+1) * digit_size] = x_test[sample]\n",
        "    figure[digit_size:, i * digit_size: (i+1) * digit_size] = x_recon[sample]\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, ) + input_dim)\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx5Mai9emDZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation\n",
        "n = 10 #figure with n x n digits\n",
        "digit_size = 32\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "# we will sample n points randomly sampled\n",
        "\n",
        "if prior_for_qz == \"GMM\":\n",
        "    z_sample = z_density.sample(n**2)\n",
        "elif prior_for_qz == \"Gaussian\":\n",
        "    z_sample = np.random.normal(size=(n**2, latent_dim), loc=mean, scale=std)\n",
        "else:\n",
        "    print(\"Distribution not found\")\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        x_decoded = decoder.predict(np.array([z_sample[i + n * j]]))\n",
        "        digit = x_decoded.reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "            j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB2Hh9GRoa_j",
        "colab_type": "text"
      },
      "source": [
        "# Metrics Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHIjGebVofW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FID Score\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "from skimage.transform import resize\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "# Functions needed to compute FID score\n",
        "def scale_images(images, new_shape): # Scale an image in a new shape using NN Interpolation\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn np.asarray(images_list)\n",
        "\n",
        "\n",
        "def calculate_fid(model, images1, images2): # Calculate Frechet Inception Distance between images1, images2\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "\n",
        "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "\tif np.iscomplexobj(covmean): # Check if the sqrtm is complex\n",
        "\t\tcovmean = covmean.real\n",
        "\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "if prior_for_qz == \"GMM\":\n",
        "    z_sample = z_density.sample(sample_size)\n",
        "elif prior_for_qz == \"Gaussian\":\n",
        "    z_sample = np.random.normal(size=(sample_size, latent_dim), mean=mean, scale=std)\n",
        "else:\n",
        "    print(\"Distribution not found\")\n",
        "sample = np.random.randint(0, len(x_test), size=sample_size)\n",
        "x_gen = decoder.predict(z_sample)\n",
        "x_real = x_test[sample]\n",
        "\n",
        "x_gen = evaluate.scale_images(x_gen, (299, 299, 1))\n",
        "x_real = evaluate.scale_images(x_real, (299, 299, 1))\n",
        "print('Scaled', x_gen.shape, x_real.shape)\n",
        "\n",
        "x_gen_t = preprocess_input(x_gen)\n",
        "x_real_t = preprocess_input(x_real)\n",
        "\n",
        "x_gen = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "x_real = np.zeros(shape=(sample_size, 299, 299, 3))\n",
        "for i in range(3):\n",
        "    x_gen[:, :, :, i] = x_gen_t[:, :, :, 0]\n",
        "    x_real[:, :, :, i] = x_real_t[:, :, :, 0]\n",
        "print('Final', x_gen.shape, x_real.shape)\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "\n",
        "# fid between images1 and images2\n",
        "fid = evaluate.calculate_fid(model, x_real, x_gen)\n",
        "print('FID (different): %.3f' % fid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}